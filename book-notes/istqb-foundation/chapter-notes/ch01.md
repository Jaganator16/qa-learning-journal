> Some definitions and examples are paraphrased closely from the ISTQB Foundation syllabus and supporting text, used here for learning purposes.

## Current level of understanding questions

**SA2 – K1**  
A non-functional requirement concerning performance is missing from the system documentation. How should this be described?  
d. It is an error — **WRONG**  
c. It is a fault — **CORRECT**

**SA2 – K1**  
Which of the following gives MOST independence in testing?  
b. Testing performed by an external consultancy — **CORRECT**

**SA3 – K1**  
Running the same set of tests will not continue to find new defects. Which of the seven testing principles does this illustrate?  
d. Pesticide paradox — **CORRECT**

---

## Why software fails

- Airbus issue shows a lack of communication between design and manufacture.
- UK government’s online tax return system was improperly tested.
- Top 10 criminals displayed on a website: non-functional requirements concerning performance were not considered before advertising the site.
- Mapping app was improperly reviewed.
- Blackout: tests were not run before implementation.

An error leads to a defect, which can cause a failure.

Why systems fail – environmental factors: radiation, magnetism, electronic fields, or pollution.

Errors need to be found and rectified, which testing contributes towards.  
Testing needs to begin when errors begin, at the very beginning of development.

Incorrect software can harm people, companies, and the environment.

---

## Keeping software under control

Common theme between examples of failures: insufficient testing, or the wrong type of testing.

Exhaustive testing of complex systems is not possible.

The new mapping app launched with a new phone and new hardware. Tens of thousands of users, who were experts on their towns, effectively generated “test cases” showing that problems existed.

If every possible test had been run, there wouldn’t have been problems; however, testing would still be running now and no product would have been launched.

It is impossible to test even moderately complex systems exhaustively.  
With complex systems, not enough testing will always be done. In this case, the problem was that the right sort of testing was not done.

---

## Testing and risk

We would expect to test an automatic flight system more than a video game because the risk is greater. There is a higher probability of failure in complex systems and the impact of failure is greater.

What we test and how we test must relate to risk.  
Greater risk means more and better testing.

---

## Testing quality

Quality is hard to define. A system meeting user requirements is a good starting point.

The resource triangle has vertices of time, quality, and money, all affecting one another and influencing which features are included or excluded.

One role of testing is to ensure that key functional and non-functional requirements are examined before the system enters service.

Testing cannot directly remove defects or enhance quality, but reporting defects makes their removal possible and contributes to improved quality.

Systematic testing allows some aspects of software quality to be measured.

---

## Deciding when “enough is enough”

We cannot test everything. Every system is subject to risk, and there is a level of quality that is acceptable. These factors are used to decide how much testing to do.

Prioritisation is the most important aspect of achieving an acceptable result with limited testing time. Important tests should be done first.

The next most important aspect is setting objective completion criteria to decide when it is safe to stop testing. These criteria define how much of the software is tested and how many defects are tolerable.

---

## Check of understanding

1. **Describe the interaction between errors, defects, and failures.**  
   Errors lead to defects, which cause failures.

2. **Give three consequences of software failures.**  
   Loss of money, loss of business reputation, death.

3. **What are the vertices of the “triangle of resource”?**  
   Time, money, quality.

---

## What testing is and what testing does

### Testing and debugging

Testing and debugging are different activities.

Debugging is performed by developers to identify the cause of defects and correct them. Some checking of the fix is usually done, but this may not include assessing wider system impact.

Testing is a systematic exploration of a component or system with the aim of finding and reporting defects.

Effective debugging should occur before testing begins to raise the quality of the system to a level worth testing.

---

## Static testing and dynamic testing

Static testing involves examining artefacts without executing code, such as reviewing documentation. Errors found at this stage are cheaper to fix than defects or failures.

Static testing uses techniques such as reviews, which help prevent defects by removing ambiguities and errors from specifications.

Dynamic testing involves executing the software under test using test data.

---

## Testing as a process

Before test execution, preparation is needed to design and set up tests. After execution, results must be recorded and evaluated.

It is essential to define test objectives. A test designed to give confidence behaves differently from one designed to find as many defects as possible.

Defining a test process ensures critical steps are not missed.

---

## Testing as a set of techniques

Effective testing finds defects and creates opportunities to improve quality. Tests that find no defects still consume resources.

Good test design uses proven techniques and principles. Even when rigorous design is not possible, general principles can still guide testing.

---

## General testing principles

- **Testing shows the presence of bugs**  
  Testing cannot prove the absence of defects. A system may have no functional defects yet still fail due to non-functional issues.

- **Exhaustive testing is impossible**  
  The number of possible data combinations makes exhaustive testing impractical for all but trivial systems.

- **Early testing**  
  Testing should begin as early as possible. Reviews of requirements help prevent defects from propagating through later stages.

- **Defect clustering**  
  Most defects tend to be found in a small number of modules (Pareto principle). Testing should focus on these areas without ignoring others.

- **Pesticide paradox**  
  Repeating the same tests reduces effectiveness. Test sets must evolve.

- **Testing is context dependent**  
  Different systems require different testing approaches.

- **Absence of errors fallacy**  
  Software with no known defects may still not meet user expectations.

---

## Fundamental test process

The five parts of the fundamental test process are:

1. Planning and control  
2. Analysis and design  
3. Implementation and execution  
4. Evaluating exit criteria and reporting  
5. Test closure activities

These activities are sequential but not rigid and may overlap.

---

## The psychology of testing

Testing is more effective when performed by someone other than the code author.

Developers aim to prove software works; testers aim to show it does not.

Greater independence improves effectiveness but increases cost.

Communication must be objective and constructive. Defects should be raised against the software, not the individual.

---

## Code of ethics

Certified software testers must handle confidential information responsibly and act ethically across areas including the public, clients, employers, colleagues, and the profession.

---

## Example examination questions

(Answers retained as-is — all marked correct)

**Example examination score:** 19/19
